{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "# 창 띄우지 않기\n",
    "options.add_argument('--headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(width=1024, height=990)\n",
    "\n",
    "# 큐레이터 추천 소장품 검색 페이지\n",
    "base_url = f'https://www.museum.go.kr/site/main/relic/recommend/list'\n",
    "# driver(chrome) open\n",
    "driver.get(base_url)\n",
    "# 로딩\n",
    "time.sleep(2)\n",
    "\n",
    "# 마지막 페이지 얻어오기\n",
    "last_page = driver.find_element(By.CLASS_NAME, 'allPage').text.split(' ')[3]\n",
    "last_page = int(last_page)\n",
    "print(\"마지막 페이지 : \", last_page)\n",
    "\n",
    "# 페이지별로 데이터 수집\n",
    "data = []\n",
    "last_page_before_fail = []\n",
    "for n in tqdm(range(1, last_page+1)) :\n",
    "    print(f'{n}페이지 유물 정보 수집중..')\n",
    "    page_url = f'https://www.museum.go.kr/site/main/relic/recommend/list?cp={n}'\n",
    "    # 탭 생성\n",
    "    driver.execute_script(\"window.open('');\")\n",
    "    # 가장 끝 탭으로 이동\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    # 링크 이동\n",
    "    driver.get(page_url)\n",
    "    time.sleep(random.randint(2, 6))\n",
    "\n",
    "    try :\n",
    "        # 해당 페이지의 유물 정보 받아오기\n",
    "        soup = bs(driver.page_source, \"html.parser\")\n",
    "        cards = soup.find_all(\"li\", class_=\"card\")\n",
    "    except Exception as e :\n",
    "        print(f'페이지 내 <li> 받아오기 실패 --> {e}')\n",
    "    \n",
    "    # 진행상황 체크용\n",
    "    card_cnt = 0\n",
    "    for card in tqdm(cards) :\n",
    "        card_cnt += 1\n",
    "        print(f'{n}페이지 내 {card_cnt}번째 유물 데이터 수집 중...')\n",
    "        try :\n",
    "            tag_a = card.find(\"a\", class_=\"img-box\")\n",
    "            title = card.find('div',class_='k-cura-txt').find('a').get_text()\n",
    "            country_and_era = card.find('strong', string='국적/시대').find_next_sibling('p').get_text(strip=True)\n",
    "            size_and_category = card.find('strong', string='크기/지정구분').find_next_sibling('p').get_text(separator='\\n', strip=True)\n",
    "            href = tag_a['href']\n",
    "            link = f\"https://www.museum.go.kr{href}\"\n",
    "            # print(\"콘텐츠 링크 : \", link)\n",
    "\n",
    "            # 새 창 열기\n",
    "            driver.execute_script(\"window.open('');\")\n",
    "            time.sleep(1)\n",
    "            # 창 이동\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            time.sleep(random.randint(2, 5))\n",
    "\n",
    "            # 각 유물들 상세페이지 이동 후 데이터 수집 \n",
    "            driver.get(link)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # BeautifulSoup로 html parser 이용\n",
    "            soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "            # h5-소제목, p-본문 데이터 추출\n",
    "            titles_and_paragraphs = soup.select('.prg > h5, .prg > p')\n",
    "            \n",
    "            # 소제목 초기화\n",
    "            current_subtitle = '' \n",
    "\n",
    "            for item in titles_and_paragraphs : \n",
    "                # 소제목이 나오면 새로운 행에 딕셔너리 데이터 추가\n",
    "                if item.name == 'h5':\n",
    "                    current_subtitle = item.get_text(strip=True)\n",
    "                    data.append({\n",
    "                        'title' : title,\n",
    "                        'era' : country_and_era,\n",
    "                        'info' : size_and_category,\n",
    "                        'description' : current_subtitle + '\\n'\n",
    "                    })\n",
    "                # 본문이 나오고 저장된 소제목이 있으면 마지막 description에 본문 추가\n",
    "                elif item.name == 'p' and current_subtitle:\n",
    "                    data[-1]['description'] += item.get_text(strip=True)\n",
    "                # 본문이 나왔으나 소제목이 없는 경우\n",
    "                elif item.name == 'p' and not current_subtitle:\n",
    "                    # 1500자 미만이면 마지막 decription에 덧붙이고\n",
    "                    if len(data[-1]['description']) < 1500 :\n",
    "                        data[-1]['description'] += item.get_text(strip=True)\n",
    "                    # 1500자 이상이면 새로운 행에 딕셔너리 데이터 추가 (토큰 수 제한 피하고자)\n",
    "                    else :\n",
    "                        data.append({\n",
    "                            'title' : title,\n",
    "                            'era' : country_and_era,\n",
    "                            'info' : size_and_category,\n",
    "                            'description' : item.get_text(strip=True)\n",
    "                        })\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            time.sleep(2)\n",
    "\n",
    "        except Exception as e2 :\n",
    "            print(f'상세페이지 내 오류 발생 --> {e2}')\n",
    "            last_page_before_fail.append(link)\n",
    "   \n",
    "\n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    # break\n",
    "\n",
    "# 데이터 프레임 생성 후 저장\n",
    "df = pd.DataFrame(columns=['title', 'era', 'info', 'description'], data=data)\n",
    "df.to_csv('./files/museum_passage.csv', encoding='utf-8-sig')\n",
    "df.to_json('./files/museum_passage.json', force_ascii=False, orient='index', indent=4)\n",
    "df.to_json(\"./files/museum_passage.jsonl\", orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "if last_page_before_fail :\n",
    "    fail_df = pd.DataFrame(last_page_before_fail).to_csv('./files/fail_log.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
